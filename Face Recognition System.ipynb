{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Student info saved: 564, Sorabh, 1234567, sorabh@gmail.com\n",
      "ğŸ”„ Capturing faces... Please stay still.\n",
      "âœ… Image 1/100 captured...\n",
      "âœ… Image 2/100 captured...\n",
      "âœ… Image 3/100 captured...\n",
      "âœ… Image 4/100 captured...\n",
      "âœ… Image 5/100 captured...\n",
      "âœ… Image 6/100 captured...\n",
      "âœ… Image 7/100 captured...\n",
      "âœ… Image 8/100 captured...\n",
      "âœ… Image 9/100 captured...\n",
      "âœ… Image 10/100 captured...\n",
      "âœ… Image 11/100 captured...\n",
      "âœ… Image 12/100 captured...\n",
      "âœ… Image 13/100 captured...\n",
      "âœ… Image 14/100 captured...\n",
      "âœ… Image 15/100 captured...\n",
      "âœ… Image 16/100 captured...\n",
      "âœ… Image 17/100 captured...\n",
      "âœ… Image 18/100 captured...\n",
      "âœ… Image 19/100 captured...\n",
      "âœ… Image 20/100 captured...\n",
      "âœ… Image 21/100 captured...\n",
      "âœ… Image 22/100 captured...\n",
      "âœ… Image 23/100 captured...\n",
      "âœ… Image 24/100 captured...\n",
      "âœ… Image 25/100 captured...\n",
      "âœ… Image 26/100 captured...\n",
      "âœ… Image 27/100 captured...\n",
      "âœ… Image 28/100 captured...\n",
      "âœ… Image 29/100 captured...\n",
      "âœ… Image 30/100 captured...\n",
      "âœ… Image 31/100 captured...\n",
      "âœ… Image 32/100 captured...\n",
      "âœ… Image 33/100 captured...\n",
      "âœ… Image 34/100 captured...\n",
      "âœ… Image 35/100 captured...\n",
      "âœ… Image 36/100 captured...\n",
      "âœ… Image 37/100 captured...\n",
      "âœ… Image 38/100 captured...\n",
      "âœ… Image 39/100 captured...\n",
      "âœ… Image 40/100 captured...\n",
      "âœ… Image 41/100 captured...\n",
      "âœ… Image 42/100 captured...\n",
      "âœ… Image 43/100 captured...\n",
      "âœ… Image 44/100 captured...\n",
      "âœ… Image 45/100 captured...\n",
      "âœ… Image 46/100 captured...\n",
      "âœ… Image 47/100 captured...\n",
      "âœ… Image 48/100 captured...\n",
      "âœ… Image 49/100 captured...\n",
      "âœ… Image 50/100 captured...\n",
      "âœ… Image 51/100 captured...\n",
      "âœ… Image 52/100 captured...\n",
      "âœ… Image 53/100 captured...\n",
      "âœ… Image 54/100 captured...\n",
      "âœ… Image 55/100 captured...\n",
      "âœ… Image 56/100 captured...\n",
      "âœ… Image 57/100 captured...\n",
      "âœ… Image 58/100 captured...\n",
      "âœ… Image 59/100 captured...\n",
      "âœ… Image 60/100 captured...\n",
      "âœ… Image 61/100 captured...\n",
      "âœ… Image 62/100 captured...\n",
      "âœ… Image 63/100 captured...\n",
      "âœ… Image 64/100 captured...\n",
      "âœ… Image 65/100 captured...\n",
      "âœ… Image 66/100 captured...\n",
      "âœ… Image 67/100 captured...\n",
      "âœ… Image 68/100 captured...\n",
      "âœ… Image 69/100 captured...\n",
      "âœ… Image 70/100 captured...\n",
      "âœ… Image 71/100 captured...\n",
      "âœ… Image 72/100 captured...\n",
      "âœ… Image 73/100 captured...\n",
      "âœ… Image 74/100 captured...\n",
      "âœ… Image 75/100 captured...\n",
      "âœ… Image 76/100 captured...\n",
      "âœ… Image 77/100 captured...\n",
      "âœ… Image 78/100 captured...\n",
      "âœ… Image 79/100 captured...\n",
      "âœ… Image 80/100 captured...\n",
      "âœ… Image 81/100 captured...\n",
      "âœ… Image 82/100 captured...\n",
      "âœ… Image 83/100 captured...\n",
      "âœ… Image 84/100 captured...\n",
      "âœ… Image 85/100 captured...\n",
      "âœ… Image 86/100 captured...\n",
      "âœ… Image 87/100 captured...\n",
      "âœ… Image 88/100 captured...\n",
      "âœ… Image 89/100 captured...\n",
      "âœ… Image 90/100 captured...\n",
      "âœ… Image 91/100 captured...\n",
      "âœ… Image 92/100 captured...\n",
      "âœ… Image 93/100 captured...\n",
      "âœ… Image 94/100 captured...\n",
      "âœ… Image 95/100 captured...\n",
      "âœ… Image 96/100 captured...\n",
      "âœ… Image 97/100 captured...\n",
      "âœ… Image 98/100 captured...\n",
      "âœ… Image 99/100 captured...\n",
      "âœ… Image 100/100 captured...\n",
      "âœ… Dataset collection completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def face_cropped(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None  # No face detected\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        return img[y:y+h, x:x+w]  # Return the first detected face\n",
    "\n",
    "def save_student_info(user_id, name, phone, email):\n",
    "    \"\"\"Save student info to students.csv\"\"\"\n",
    "    file_path = \"students.csv\"\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"ID\", \"Name\", \"Phone\", \"Email\"])  # Write header\n",
    "        writer.writerow([user_id, name, phone, email])\n",
    "\n",
    "    print(f\"âœ… Student info saved: {user_id}, {name}, {phone}, {email}\")\n",
    "\n",
    "def generate_dataset(user_id=1, name=\"Unknown\", phone=\"N/A\", email=\"N/A\", num_samples=100, delay=0.1):\n",
    "    cap = cv2.VideoCapture(0)  # Open webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not access the camera.\")\n",
    "        exit()\n",
    "\n",
    "    img_id = 0\n",
    "    os.makedirs(\"data\", exist_ok=True)  # Ensure \"data\" folder exists\n",
    "\n",
    "    # Save student info\n",
    "    save_student_info(user_id, name, phone, email)\n",
    "\n",
    "    print(\"ğŸ”„ Capturing faces... Please stay still.\")\n",
    "\n",
    "    while img_id < num_samples:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"âŒ Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        face = face_cropped(frame)\n",
    "        if face is not None:\n",
    "            img_id += 1\n",
    "            face = cv2.resize(face, (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            file_name_path = os.path.join(\"data\", f\"user_{user_id}_{img_id}.jpg\")\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "\n",
    "            # Display capture progress\n",
    "            cv2.putText(face, f\"ID {user_id} - {img_id}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Capturing Face\", face)\n",
    "\n",
    "            print(f\"âœ… Image {img_id}/{num_samples} captured...\")\n",
    "\n",
    "            time.sleep(delay)  # Slow down the capture process\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or img_id == num_samples:\n",
    "            print(\"âœ… Dataset collection completed successfully.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    user_id = input(\"Enter user ID: \")\n",
    "    name = input(\"Enter Name: \")\n",
    "    phone = input(\"Enter Phone Number: \")\n",
    "    email = input(\"Enter Email: \")\n",
    "    generate_dataset(user_id=int(user_id), name=name, phone=phone, email=email, num_samples=100, delay=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Training model... Please wait.\n",
      "âœ… Model training completed! Model saved as 'trained_model.yml'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_model(data_folder=\"data\", model_filename=\"trained_model.yml\"):\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()  # Create LBPH Face Recognizer\n",
    "    faces = []\n",
    "    ids = []\n",
    "\n",
    "    # Get the list of images from the dataset\n",
    "    image_files = [f for f in os.listdir(data_folder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"âŒ No images found in dataset. Please collect face samples first.\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ”„ Training model... Please wait.\")\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(data_folder, image_file)\n",
    "\n",
    "        # Read image in grayscale\n",
    "        face_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Extract user ID from filename (e.g., \"user_1_10.jpg\" â†’ ID = 1)\n",
    "        user_id = int(image_file.split(\"_\")[1])  # Extract ID from filename\n",
    "        \n",
    "        faces.append(face_img)\n",
    "        ids.append(user_id)\n",
    "\n",
    "    # Convert ID list to numpy array\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    # Train the face recognizer\n",
    "    recognizer.train(faces, ids)\n",
    "\n",
    "    # Save the trained model\n",
    "    recognizer.save(model_filename)\n",
    "\n",
    "    print(f\"âœ… Model training completed! Model saved as '{model_filename}'.\")\n",
    "\n",
    "# Run the training function\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attendance Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded trained model from 'trained_model.yml'.\n",
      "ğŸ“· Face recognition started. Press 'Enter' to exit.\n",
      "âœ… Face recognition session ended.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Load Haar cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load trained model\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "model_filename = \"trained_model.yml\"\n",
    "\n",
    "if os.path.exists(model_filename):\n",
    "    recognizer.read(model_filename)\n",
    "    print(f\"âœ… Loaded trained model from '{model_filename}'.\")\n",
    "else:\n",
    "    print(f\"âŒ Model file '{model_filename}' not found! Please train the model first.\")\n",
    "    exit()\n",
    "\n",
    "# Load student details from CSV\n",
    "student_data = {}\n",
    "csv_file = \"students.csv\"\n",
    "if os.path.exists(csv_file):\n",
    "    with open(csv_file, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            student_data[int(row[0])] = {\"name\": row[1], \"phone\": row[2], \"email\": row[3]}\n",
    "else:\n",
    "    print(f\"âŒ Student database '{csv_file}' not found!\")\n",
    "\n",
    "def recognize_face(frame):\n",
    "    \"\"\"Detect and recognize faces in a video frame.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        id, confidence = recognizer.predict(face_roi)\n",
    "\n",
    "        confidence = int(100 * (1 - confidence / 300))\n",
    "\n",
    "        if confidence > 75 and id in student_data:\n",
    "            student = student_data[id]\n",
    "            color = (0, 255, 0)  # Green for known users\n",
    "        else:\n",
    "            name = \"UNKNOWN\"\n",
    "            color = (0, 0, 255)  # Red for unknown users\n",
    "\n",
    "        # Draw rectangle and text\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame,  f\"{name} ({confidence}%)\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open webcam for real-time recognition\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Error: Could not access the camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ“· Face recognition started. Press 'Enter' to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"âŒ Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    frame = recognize_face(frame)\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 13:  # Press 'Enter' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"âœ… Face recognition session ended.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
